# CODI Copilot Instructions

- **Purpose**: CODI trains LoRA-adapted causal LMs to generate implicit CoT latents then decode final answers, with optional auxiliary decoder + distillation from a teacher run on reference inputs ([src/model.py](src/model.py), [train.py](train.py)).
- **Core model**: `CODI` wraps a Hugging Face causal LM, extends vocab with `pad`/`<bot>`/`<eot>` tokens, and can attach an optional second decoder path plus a projection MLP. LoRA targets differ by base model family (llama/mistral/falcon/qwen → q/k/v/o/up/down/gate proj; phi → q/k/v/dense/fc1/fc2; gpt2 → c_attn/c_proj/c_fc).
- **Losses**: Student CE on generated answer, layer-wise distillation to teacher hidden states at answer positions, optional teacher CE on reference inputs, and optional explain/decoder loss when `use_decoder=True` (distill/CE weights configurable via TrainingArguments). Optional trajectory consistency loss constrains latent spread.
- **Latent loop**: Encoder question → hidden state seed → iterate `num_latent` steps with optional projection; final step concatenates answer tokens and runs CE + distill. Special BOT/EOT tokens delimit latent/answer segments. Trajectory loss computed over collected latent embeddings.
- **Data expectations**: Datasets are loaded from absolute JSON paths under `/mnt/shared-storage-user/.../coconut/data/` with fields like `question`, `cot`/`steps`, and `answer`. `include_last_cot`, `max_token_num`, and `num_latent` gate filtering and replication. When `icot` modes are used, multiple truncated CoTs are generated; answers must end with numeric token (strip `####`).
- **Tokenization quirks**: Tokenizer is left/right padded depending on stage; pad token added if missing. In preprocessing, questions append BOT (and sometimes EOS) before decoding; reference inputs concatenate question + cot + answer to guide teacher positions. Positions are shifted by +1 for llama/qwen spacing.
- **Attention masks**: By default `ref_attention_mask` is disabled unless `fix_attn_mask=True`; dynamic masks are used to stop decoder attending padding when `use_prj`/latent steps run.
- **Projection path**: `use_prj` inserts a GELU MLP (dim→prj_dim→dim with optional LayerNorm). `soft_weight` (in test/probe) blends latent with soft vocab embeddings.
- **Decoder path**: If `use_decoder=True`, `get_steps` extracts latent spans between start/end markers (LLAMA defaults 2501/1134 → 2511; GPT tokens differ). Steps are padded, embedded, optionally projected (`pj_in`/`pj_out`), and trained via CE against shifted labels (masking pad and sentinel -570 prefixes).
- **Trajectory consistency**: Optional `use_trajectory_consistency=True` adds a geometric constraint forcing latent embeddings to cluster around their Fréchet mean (centripetal force). Supports Euclidean (arithmetic mean + L2 distance) and hyperbolic spaces (Karcher mean + Poincaré distance). Configured via `trajectory_space_type`, `trajectory_radius_threshold`, `trajectory_loss_factor`, and `trajectory_curvature`. Implementation in [src/trajectory_consistency.py](src/trajectory_consistency.py) computes center via iterative gradient descent for hyperbolic case, penalizes tokens exceeding radius.
- **Evaluation flows**: [test.py](test.py) runs GSM8K/MultiArith/SVAMP/Commonsense QA; loads LoRA checkpoint from `ckpt_dir` (safetensors preferred) and uses sampling (temp/top-k/p) with repeated latent iterations. Writes decoded latents/logs to `outputs/decoded_latent.txt` and reports accuracy.
- **Probing**: [probe_latent_token.py](probe_latent_token.py) variant inspects top-k tokens at each latent step and optionally logs attended tokens for successful samples.
- **Training**: `train.py` builds `SupervisedDataset` from JSON, masks source tokens with -100, pads decoder labels, and uses `CustomTrainer` to log CE/distill/ref losses every `logging_steps`. LoRA config constructed from `model_args` flags; tokenizer max length is capped at 256 during tokenization in preprocessing, not full `model_max_length`.
- **Scripts**: Shell helpers under [scripts/](scripts) (e.g., `train_gpt2_gsm8k-aug.sh`, `test_llama1b.sh`) show typical arg sets (data_name, model_name_or_path, lora flags, inf_latent_iterations). Mirror those when adding new runs.
- **Precision & devices**: Defaults to full precision unless `full_precision=False` triggers 4-bit quantization via bitsandbytes; runtime casts to bfloat16 in inference paths. Assume CUDA and adjust `device_map`/precision only if necessary.
- **Checkpoints**: During evaluation, adapter weights are loaded via `ckpt_dir` then tied; ensure tokenizer matches the base model path (`model_name_or_path`) used for training.
- **Outputs/logging**: `print_loss` controls verbose loss logging; `log_full` unused currently. Distillation/std normalisation toggled by `distill_loss_div_std`.
- **Extending**: When adding model families, update LoRA target module selection; when altering data formats, ensure `get_steps` markers and `answer` parsing remain consistent with downstream evaluation expecting numeric extraction via regex.
- **Commands (examples)**:
  - Train (edit paths/tokens): `python train.py --model_name_or_path mistralai/Mistral-7B-Instruct-v0.2 --data_name icot --lora_init True --num_latent 5 --bf16 True --per_device_train_batch_size 1`
  - Train with trajectory consistency: `python train.py --model_name_or_path mistralai/Mistral-7B-Instruct-v0.2 --data_name icot --lora_init True --num_latent 5 --use_trajectory_consistency True --trajectory_space_type euclidean --trajectory_radius_threshold 2.0 --trajectory_loss_factor 0.1`
  - Test trajectory implementation: `python test_trajectory_consistency.py`
  - Eval GSM8K: `python test.py --model_name_or_path mistralai/Mistral-7B-Instruct-v0.2 --ckpt_dir <ckpt> --data_name gsm8k --batch_size 4 --inf_latent_iterations 3`
- **Gotchas**: Absolute data paths in code may break in new envs—parameterize or symlink. Ensure pad/EOS/BOT/EOT tokens persist when swapping tokenizers. `remove_eos` changes how BOT/EOT are prefixed and how answers are assembled; keep consistent between train/test.
